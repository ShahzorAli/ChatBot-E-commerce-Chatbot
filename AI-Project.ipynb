{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7076\\2020374956.py:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning dataset saved to C:\\Users\\HP\\Downloads\\new_fine_tuning_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# File path to your dataset\n",
    "file_path = r\"C:\\\\Users\\\\HP\\\\Downloads\\\\train.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove special characters, multiple spaces, and make lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,\\s]\", \"\", str(text))\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Clean the Question and Answer columns\n",
    "data['Question'] = data['Question'].apply(clean_text)\n",
    "data['Answer'] = data['Answer'].apply(clean_text)\n",
    "\n",
    "# Prepare dataset for fine-tuning\n",
    "fine_tuning_data = [\n",
    "    {\"question\": row['Question'], \"answer\": row['Answer']}\n",
    "    for _, row in data.iterrows()\n",
    "]\n",
    "\n",
    "# Save to JSON for fine-tuning\n",
    "output_path = \"C:\\\\Users\\\\HP\\\\Downloads\\\\new_fine_tuning_data.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(fine_tuning_data, f, indent=4)\n",
    "\n",
    "\n",
    "output_path = r\"C:\\Users\\HP\\Downloads\\new_fine_tuning_data.json\"\n",
    "print(f\"Fine-tuning dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60be7077c1b244658481b8a1cf45658c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_100\\895148788.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 13:37:15, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.934616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.886226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.577015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.762497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.400100</td>\n",
       "      <td>0.703343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.400100</td>\n",
       "      <td>0.663743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.400100</td>\n",
       "      <td>0.600286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.400100</td>\n",
       "      <td>0.545678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.400100</td>\n",
       "      <td>0.505268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.400100</td>\n",
       "      <td>0.463468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.432764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.411184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.388009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.376138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.369556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.361745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.355144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.352092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.349440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.346225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.343491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.341986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.341162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.340839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.339395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.337460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.336078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.333973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.332120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.331488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.331219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.332088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.331110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.330370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.329988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.329922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.329758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.329409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.329023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.328616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.328439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.328981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.329002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.328884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.328762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.328802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.328786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.328786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "json_path = \"C:/Users/HP/Downloads/dataset.json\"\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the dataset to extract 'question' and 'answer' columns\n",
    "questions = data['questions']\n",
    "data_dict = {\n",
    "    'question': [item['question'] for item in questions],\n",
    "    'answer': [item['answer'] for item in questions]\n",
    "}\n",
    "\n",
    "# Create the dataset from the data\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Step 2: Load Pre-trained Model and Tokenizer\n",
    "model_name = \"t5-base\"  # t5 base for best results\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Step 3: Preprocess Data\n",
    "def preprocess_data(examples):\n",
    "    inputs = [f\"question: {q}\" for q in examples[\"question\"]]\n",
    "    targets = [f\"answer: {a}\" for a in examples[\"answer\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "data_split = tokenized_data.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = data_split[\"train\"]\n",
    "eval_dataset = data_split[\"test\"]\n",
    "\n",
    "# Step 4: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               # Directory to save checkpoints\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                   # Use a lower learning rate for quicker training\n",
    "    per_device_train_batch_size=8,        # Batch size for training \n",
    "    per_device_eval_batch_size=8,         # Batch size for evaluation\n",
    "    num_train_epochs=50,                   # Reduce epochs for faster results\n",
    "    save_steps=500,                       # Save checkpoint every 500 steps\n",
    "    save_total_limit=1,                   # Save only the most recent checkpoint\n",
    "    logging_dir=\"./logs\",                  # Directory for logs\n",
    "    logging_steps=50,                     # Log every 50 steps\n",
    "    fp16=torch.cuda.is_available(),       \n",
    "    dataloader_num_workers=2              # Fewer workers for faster data loading\n",
    ")\n",
    "\n",
    "# Step 5: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Step 6: Train the Model\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./new_fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./new_fine_tuned_model\")\n",
    "print(\"Model fine-tuned and saved!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "model_path = \"C:/Users/HP/Downloads/new_fine_tuned_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import heapq\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load Fine-Tuned T5 Model\n",
    "model_path = \"./new_fine_tuned_model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Load Pre-trained SentenceTransformer for Semantic Similarity\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Dataset file location\n",
    "dataset_file = r\"C:\\Users\\HP\\Downloads\\dataset.json\"\n",
    "\n",
    "# Function to load the knowledge base\n",
    "def load_knowledge_base(dataset_file):\n",
    "    \"\"\"Load the knowledge base dynamically from a JSON file.\"\"\"\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'questions' in data and isinstance(data['questions'], list) and all('question' in entry and 'answer' in entry for entry in data['questions']):\n",
    "        return data['questions']\n",
    "    else:\n",
    "        raise ValueError(\"The dataset format is incorrect. It must have a 'questions' key with a list of dictionaries containing 'question' and 'answer' fields.\")\n",
    "\n",
    "# Function: Generate Initial Response using T5\n",
    "def generate_response(question, model, tokenizer, max_length=50):\n",
    "    inputs = tokenizer(f\"question: {question}\", return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_beams=5, early_stopping=True, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Function: A* Search for Knowledge Retrieval\n",
    "def a_star_search(query, dataset_file, threshold=0.5):\n",
    "    knowledge_base = load_knowledge_base(dataset_file)\n",
    "    questions = [entry[\"question\"] for entry in knowledge_base]\n",
    "    kb_embeddings = embedding_model.encode(questions, convert_to_tensor=True)\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    query_embedding = query_embedding / query_embedding.norm()  # Normalize query embedding\n",
    "\n",
    "    pq = []\n",
    "    for idx, entry in enumerate(knowledge_base):\n",
    "        g = util.pytorch_cos_sim(query_embedding, kb_embeddings[idx])[0][0].item()\n",
    "        if g >= threshold:\n",
    "            f = 1 - g  # Cost function\n",
    "            heapq.heappush(pq, (f, idx, entry))\n",
    "\n",
    "    if not pq:\n",
    "        return \"No relevant match found in the knowledge base.\"\n",
    "    \n",
    "    _, _, best_entry = heapq.heappop(pq)\n",
    "    return best_entry[\"answer\"]\n",
    "\n",
    "\n",
    "\n",
    "def generate_variations(response):\n",
    "    words = response.split()\n",
    "    variations = []\n",
    "    \n",
    "    # List of excluded words/phrases to preserve\n",
    "    excluded_words = [\n",
    "        'customer', 'support', 'team', 'item', 'order', 'return', 'ship', 'contact', 'assist', 'necessary', 'steps'\n",
    "    ]\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() in excluded_words:  # Skip excluded words\n",
    "            continue\n",
    "        \n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            for synonym in synonyms[:3]:  # Limit to top 3 synonyms\n",
    "                new_word = synonym.lemmas()[0].name()\n",
    "                # Ensure the new word is different and makes sense in context\n",
    "                if new_word != word and new_word not in excluded_words:\n",
    "                    new_words = words[:]\n",
    "                    new_words[i] = new_word  # Replace the word with the synonym\n",
    "                    variation = \" \".join(new_words)\n",
    "                    \n",
    "                    # Check semantic similarity to ensure context\n",
    "                    variation_score = util.pytorch_cos_sim(\n",
    "                        embedding_model.encode(response, convert_to_tensor=True),\n",
    "                        embedding_model.encode(variation, convert_to_tensor=True)\n",
    "                    )[0][0].item()\n",
    "                    \n",
    "                    if variation_score > 0.8:  # Accept only contextually similar variations\n",
    "                        variations.append(variation)\n",
    "    return variations\n",
    "\n",
    "def hill_climbing(query, response1, response2, embedding_model, max_iterations=10):\n",
    "    def score(response):\n",
    "        query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "        response_embedding = embedding_model.encode(response, convert_to_tensor=True)\n",
    "        return util.pytorch_cos_sim(query_embedding, response_embedding)[0][0].item()\n",
    "\n",
    "    response1_score = score(response1)\n",
    "    response2_score = score(response2)\n",
    "\n",
    "    if response1_score >= response2_score:\n",
    "        current_response = response1\n",
    "        current_score = response1_score\n",
    "    else:\n",
    "        current_response = response2\n",
    "        current_score = response2_score\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        neighbors = generate_variations(current_response)\n",
    "        best_neighbor = current_response\n",
    "        best_score = current_score\n",
    "        \n",
    "        for neighbor in neighbors:\n",
    "            neighbor_score = score(neighbor)\n",
    "            if neighbor_score > best_score:\n",
    "                best_neighbor = neighbor\n",
    "                best_score = neighbor_score\n",
    "\n",
    "        if best_score > current_score:\n",
    "            current_response = best_neighbor\n",
    "            current_score = best_score\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return current_response\n",
    "\n",
    "\n",
    "def chatbot_pipeline(user_query, dataset_file, model, tokenizer):\n",
    "    retrieved_response = a_star_search(user_query, dataset_file, threshold=0.5)\n",
    "    if not retrieved_response:\n",
    "        retrieved_response = \"No relevant match found in the knowledge base.\"\n",
    "\n",
    "    fine_tuned_response = generate_response(user_query, model, tokenizer)\n",
    "    optimized_response = hill_climbing(user_query, retrieved_response, fine_tuned_response, embedding_model)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_response\": retrieved_response,\n",
    "        \"fine_tuned_response\": fine_tuned_response,\n",
    "        \"optimized_response\": optimized_response,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_gui():\n",
    "    window = tk.Tk()\n",
    "    window.title(\"E-Commerce Chatbot\")\n",
    "\n",
    "    # Header area\n",
    "    header = tk.Label(window, text=\"Welcome to E-Commerce Chatbot\", font=(\"Helvetica\", 16, \"bold\"), bg=\"#4CAF50\", fg=\"white\", padx=10, pady=10)\n",
    "    header.pack(fill=tk.X)\n",
    "\n",
    "    # Chat history area\n",
    "    chat_history = scrolledtext.ScrolledText(window, width=80, height=20, wrap=tk.WORD, font=(\"Arial\", 12), bg=\"#f0f0f0\")\n",
    "    chat_history.pack(padx=10, pady=10)\n",
    "    chat_history.config(state=tk.DISABLED)\n",
    "\n",
    "    # User input area\n",
    "    user_input_frame = tk.Frame(window)\n",
    "    user_input_frame.pack(pady=10)\n",
    "\n",
    "    user_input = tk.Entry(user_input_frame, width=70, font=(\"Arial\", 12))\n",
    "    user_input.grid(row=0, column=0, padx=10)\n",
    "\n",
    "    # Dynamic status label\n",
    "    status_label = tk.Label(window, text=\"\", font=(\"Arial\", 10), fg=\"blue\")\n",
    "    status_label.pack()\n",
    "\n",
    "    def update_status(message):\n",
    "        status_label.config(text=message)\n",
    "        window.update_idletasks()\n",
    "\n",
    "    # Function to handle user input and responses\n",
    "    def send_message():\n",
    "        user_message = user_input.get()\n",
    "        if not user_message:\n",
    "            messagebox.showwarning(\"Input Error\", \"Please enter a query!\")\n",
    "            return\n",
    "\n",
    "        if user_message.lower() == \"exit\":\n",
    "            if messagebox.askyesno(\"Exit\", \"Are you sure you want to exit?\"):\n",
    "                window.quit()\n",
    "            return\n",
    "\n",
    "        # Display user message\n",
    "        chat_history.config(state=tk.NORMAL)\n",
    "        chat_history.insert(tk.END, f\"You: {user_message}\\n\")\n",
    "        chat_history.config(state=tk.DISABLED)\n",
    "        chat_history.yview(tk.END)\n",
    "\n",
    "        user_input.delete(0, tk.END)\n",
    "\n",
    "        # Background task for chatbot pipeline\n",
    "        def process_response():\n",
    "            update_status(\"Processing your query...\")\n",
    "            time.sleep(0.5)  # Simulate loading time\n",
    "\n",
    "            result = chatbot_pipeline(user_message, dataset_file, model, tokenizer)\n",
    "\n",
    "            chat_history.config(state=tk.NORMAL)\n",
    "            chat_history.insert(tk.END, \"--- Retrieved Response from Knowledge (A*) ---\\n\")\n",
    "            chat_history.insert(tk.END, result[\"retrieved_response\"] + \"\\n\\n\")\n",
    "            chat_history.insert(tk.END, \"--- Generated Response from Fine-Tuned Model ---\\n\")\n",
    "            chat_history.insert(tk.END, result[\"fine_tuned_response\"] + \"\\n\\n\")\n",
    "            chat_history.insert(tk.END, \"--- Final Optimized Response ---\\n\")\n",
    "            chat_history.insert(tk.END, result[\"optimized_response\"] + \"\\n\\n\")\n",
    "            chat_history.config(state=tk.DISABLED)\n",
    "            chat_history.yview(tk.END)\n",
    "\n",
    "            update_status(\"Response generated successfully!\")\n",
    "\n",
    "        threading.Thread(target=process_response).start()\n",
    "\n",
    "    # Buttons\n",
    "    send_button = tk.Button(user_input_frame, text=\"Send\", width=15, font=(\"Arial\", 12), bg=\"#4CAF50\", fg=\"white\", command=send_message)\n",
    "    send_button.grid(row=0, column=1, padx=10)\n",
    "\n",
    "    exit_button = tk.Button(window, text=\"Exit\", width=10, font=(\"Arial\", 12), bg=\"#f44336\", fg=\"white\", command=lambda: window.quit() if messagebox.askyesno(\"Exit\", \"Are you sure you want to exit?\") else None)\n",
    "    exit_button.pack(pady=10)\n",
    "\n",
    "    window.mainloop()\n",
    "\n",
    "# Start the GUI chat\n",
    "create_gui()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
